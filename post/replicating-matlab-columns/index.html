<!doctype html>
<head><meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1"/><title>Measure your code, or, Replicating columns in Matlab</title>
<link href="/atom.xml" type="application/atom+xml" rel="alternate" />
<meta name="description" content="Three-ish implementations of an array slicing-and-dicing problemâ€¦ in Matlab." />
<meta name="twitter:card" value="summary">
<meta property="og:title" content="Measure your code, or, Replicating columns in Matlab" />
<meta property="og:type" content="article" />
<meta property="og:url" content="post/replicating-matlab-columns/index.html" />
<meta property="og:image" content="https://fasiha.github.io/post/replicating-matlab-columns/worker.jpg" />
<meta property="og:description" content="Three-ish implementations of an array slicing-and-dicing problemâ€¦ in Matlab." />
<link href="../../assets/theme.css" rel="stylesheet"><style>/* Tomorrow Night Eighties Theme */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
  color: #999999;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
  color: #f2777a;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
  color: #f99157;
}

/* Tomorrow Yellow */
.hljs-attribute {
  color: #ffcc66;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
  color: #99cc99;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
  color: #6699cc;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
  color: #cc99cc;
}

.hljs {
  display: block;
  overflow-x: auto;
  background: #2d2d2d;
  color: #cccccc;
  padding: 0.5em;
}

.hljs-emphasis {
  font-style: italic;
}

.hljs-strong {
  font-weight: bold;
}
</style>

</head>
<figure class="full-width no-top"><img class="top-banner-image" src="worker.jpg" width="1500" height="372"></figure>  <ul class="top-nav">
    <li><a href="/">Blog</a></li>
    <li><a href="/#contact">Contact</a></li>
    <li><a href="/atom.xml">Feed</a></li>
  </ul><h1>Measure your code, or, Replicating columns in Matlab</h1><p><em>Updated on Tue, 31 May 2016 03:16:58 GMT, tagged with â€˜techâ€™.</em></p><p>A colleague and I needed to optimize a bottleneck function in our Matlab code, which expanded a matrix in a specific way. This post contains three-ish implementations that we wound up comparing runtime:</p>
<ol type="1">
<li>the initial code the colleague (a long-time Matlab and Python coder) had come up with,</li>
<li>a stupid and simple approach I wrote to better understand what it was doing,</li>
<li>a complicated approach that suggested itself after a few minutes of Thinking Hard, but that produced slightly wrong outputs, and</li>
<li>a tweak on #3 with correct output.</li>
</ol>
<p>At this point, youâ€™d think that #2 would be the slowest, then #1 (what we started out with), then #3 and/or #4.</p>
<p>Letâ€™s introduce our contestants.</p>
<h2 id="problem-statement-and-stupid-simple-implementation">Problem statement and stupid-simple implementation</h2>
<p>Given a 2D array with width <code>W</code>, and a <code>W</code>-long vector containing non-negative integers, expand the array by replicating each column as many times as specified in the vector.</p>
<p>Hereâ€™s the stupid-simple implementation to confirm we understand what that means:</p>
<pre class="matlab"><code class="hljs"><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">M</span> = <span class="hljs-title">stupidSimple</span><span class="hljs-params">(P, reps)</span></span>
  M = [];
  <span class="hljs-keyword">for</span> colIdx = <span class="hljs-number">1</span> : <span class="hljs-built_in">length</span>(reps)
    M = [M repmat(P(:, colIdx), <span class="hljs-number">1</span>, reps(colIdx))];
  <span class="hljs-keyword">end</span>
<span class="hljs-keyword">end</span></code></pre>
<p>Itâ€™s stupid-simple because itâ€™s expanding the output array at each iteration across the input arrayâ€”reallocation nightmare! And <code>repmat</code>, isnâ€™t that usually slow?</p>
<h2 id="the-original-code-a-good-first-effort">The original code: a good first effort?</h2>
<p>My colleague, no fool, knew reallocating each iteration was badâ€”itâ€™s a warning in the Matlab linter! Said colleague knew all about <em>index arrays</em>, and about <code>arrayfun</code>, so used the latter to create one big index array <code>i</code>:</p>
<pre class="matlab"><code class="hljs"><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">M</span> = <span class="hljs-title">initialAttempt</span><span class="hljs-params">(P, reps)</span></span>
  c = arrayfun(@(n, <span class="hljs-built_in">i</span>) <span class="hljs-built_in">i</span> * <span class="hljs-built_in">ones</span>(n, <span class="hljs-number">1</span>), reps, <span class="hljs-number">1</span> : <span class="hljs-built_in">length</span>(reps), <span class="hljs-string">'un'</span>, <span class="hljs-number">0</span>);
  <span class="hljs-built_in">i</span> = cell2mat(c(:));
  M = P(:, <span class="hljs-built_in">i</span>);
<span class="hljs-keyword">end</span></code></pre>
<p>(Matlab, Numpy/Python, Julia, all have this nice aspect to their array domain-specific language: you can index into an array with another <em>index array</em> to get a subset of slices of the array. If <code>x = [10 20 30]</code>, then <code>x([3 3 1 2])</code> is <code>[30 30 10 20]</code>. Thatâ€™s what this code is using.)</p>
<p>This was causing the bottleneck in your Bayesian estimation problem? Hmmmâ€¦</p>
<h2 id="stroke-of-lucki-mean-flash-of-genius">Stroke of luckâ€”I mean, flash of genius!</h2>
<p>I doodled with pen-and-paper for a bit, then spent more time typing something unreadable to confirm it worked, and it almost did! (It didnâ€™t handle the case when a column needed to be replicated 0 times, i.e., deleted.)</p>
<p>Hereâ€™s that something unreadable:</p>
<pre class="matlab"><code class="hljs"><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">M</span> = <span class="hljs-title">sparseSumZeroBad</span><span class="hljs-params">(P, reps)</span></span>
  widthM = sum(reps);
  z = <span class="hljs-built_in">zeros</span>(widthM, <span class="hljs-number">1</span>); <span class="hljs-comment">% vector to be cumsummed</span>

  <span class="hljs-comment">% indexes of z to be 1 (otherwise z is 0)</span>
  <span class="hljs-built_in">j</span> = [<span class="hljs-number">1</span> <span class="hljs-number">1</span> + cumsum(reps(<span class="hljs-number">1</span> : end - <span class="hljs-number">1</span>))]; <span class="hljs-comment">% ğŸŒ¶ pixie dust</span>

  z(<span class="hljs-built_in">j</span>) = <span class="hljs-number">1</span>;
  <span class="hljs-built_in">i</span> = cumsum(z);
  M = P(:, <span class="hljs-built_in">i</span>);
<span class="hljs-keyword">end</span></code></pre>
<p>The core insight is that we can use <code>cumsum</code> (cumulative sum, prefix sum, etc.) to build the index array <code>i</code> efficiently. That isâ€”if <code>reps = [4 2 3]</code>, then index array <code>i = [1 1 1 1 2 2 3 3 3]</code>, i.e., four 1s, two 2s, three 3s (confusing choice of example now, but made sense then). The code above is a way of <em>quickly</em> generating such an <code>i</code> index array by <code>cumsum</code>-ing a sparse binary array, specifically, <code>z = [1 0 0 0 1 0 1 0 0]</code>.</p>
<p>What took a while to type up was getting those magic numbers in ğŸŒ¶ to work above. ğŸŒ¶ above decides where to put the ones in the sparse array.</p>
<p>How did I come up with the correct offsets and indexes in ğŸŒ¶? Formal reasoning? Rigorous analysis? Logical proof? I confess: I just wrote the code to work for the example Iâ€™d doodled, <code>reps = [4 2 3]</code>, and it turned out to work for the general case.</p>
<p>I do this a lot and I assumed everyone else did it to: code something by making it work for one specific example that I can visualize, then test to see if it works for the general case. Itâ€™s not at all easy to explain or prove to someone why it works. Itâ€™s, letâ€™s say, the test-driven development (TDD) approach to numerical computing. Itâ€™s been working for me so far.</p>
<h2 id="tweak">Tweak</h2>
<p><code>sparseSumZeroBad</code> doesnâ€™t correctly handle the case when <code>reps</code> contains 0, i.e., where columns can be deleted, so with some more ğŸŒ¶s, I tweaked it to get <code>sparseSum</code>â€”you can <a href="test.m">read it here</a>, but it works, all three-ish functions produce the right answer, and weâ€™re approaching the moral of the story: measurement.</p>
<h2 id="you-must-measure-everything">â€œYou must measure everythingâ€</h2>
<p>I must quote the great Andrei Alexandrescu, the C++ and D author (of books and the programming languages), from his talk, â€œWriting Quick Code in C++, Quicklyâ€ at GoingNative 2013 (<a href="https://www.youtube.com/watch?v=ea5DiCg8HOY&amp;feature=youtu.be&amp;t=5m32s">Youtube</a>):</p>
<blockquote>
<p>You must measure everything. We all have intuition. And the intuition of programmers is â€¦ always wrong. Outdated. Intuition ignores a lot of aspects of a complex reality. Todayâ€™s machine architectures are so complicated, thereâ€™re so many variables in flight at any point in time that itâ€™s essentially impossible to consider them deterministic machines any more. They are not deterministic any more. <strong>So we make very often big mistakes when assuming things about whatâ€™s going to make fast code.</strong> [E.g.,] fewer instructions â‰  faster code. Data [access] is not always faster than computation. The only good intuition is â€œI should measure this stuff and see what happens.â€</p>
<p>To quote a classic, who is still alive: Walter Bight: â€œMeasuring gives you a leg up on experts who are so good they donâ€™t need to measure.â€ Walter and I have been working on optimizing bits and pieces of a project we work on [the D programming language] and â€¦ <em><strong>whenever we think we know what weâ€™re doing, we measure, and itâ€™s just the other way around.</strong></em></p>
</blockquote>
<p>Programmers might be the first profession to <em>viscerally</em> believe and appreciate Duncan Watts when he says that complex systems are ill-served by our common sense evolved to navigate your everyday social life. His book, <a href="http://everythingisobvious.com/the-book/"><em>Everything is Obviousâ€”Once You Know the Answer</em></a> is likely my favorite book, reigning champion for several years now. We get to <em>regularly</em> experience our straightforward and reasonableâ€”oh so reasonableâ€”beliefs going kaput in the face of a the complicated interactions between registers, multiple layers of cache, RAM, swap, and network.</p>
<p>And if youâ€™re not the kind of programmer who optimizes cycles, take it from Andrei and Walter when they say â€œwhenever we think we know what weâ€™re doing, we measure, and itâ€™s just the other way aroundâ€.</p>
<p>Further reading: Emil Ernerfeldtâ€™s <a href="http://www.ilikebigbits.com/blog/2014/4/21/the-myth-of-ram-part-i">â€œThe Myth of RAM, part Iâ€</a>. His <code>ram_bench</code>, on (1) my mid-2014 MacBook Pro with 16 GB RAM as well as (2) a $10â€™000 workstation with dual-socket Xeons and 384 GB RAM:</p>
<figure>
<a href="bench.svg"><img src="bench.svg" alt="ram_bench on laptop and a powerful node."></a>
<figcaption>
ram_bench on laptop and a powerful node.
</figcaption>
</figure>
<p>(<a href="https://gist.github.com/fasiha/d8b5eae8431dffb3ee14f41f167a6a2a">Raw data available</a>.) Those jumps basically mean the runtime to chew linearly through an <code>N</code>-element array might be the <em>same</em> as <code>10 * N</code> elements ğŸ˜³! Meanwhile, the runtime might jump ten-fold when the input size doubles ğŸ˜±! Gadzooks!</p>
<p>Even further reading: Igor Ostrovskyâ€™s <a href="http://igoro.com/archive/fast-and-slow-if-statements-branch-prediction-in-modern-processors/">â€œFast and slow if-statements: branch prediction in modern processorsâ€</a> on another esoteric reason for fast and slow code (and his <a href="http://igoro.com/archive/gallery-of-processor-cache-effects/">â€œGallery of Processor Cache Effectsâ€</a> is a nice recap of Emil Ernerfeldtâ€™s piece). And I rather enjoyed Chad Austinâ€™s notes on squeezing even reasonable performance out of modern CPUs in <a href="https://chadaustin.me/2017/05/writing-a-really-really-fast-json-parser/">â€œWriting a Really, Really Fast JSON Parserâ€</a>.</p>
<p>Further watching: Cliff Click, formerly of Azul Systems (where they make thousand-core JVM boxes), has a great talk called â€œA crash course in modern hardwareâ€ (2013, on <a href="https://www.infoq.com/presentations/click-crash-course-modern-hardware">InofQ</a> and <a href="https://www.youtube.com/watch?v=OyTA5EaAb3Y">Youtube</a>) where he walks through a precise timeline of a cache miss.</p>
<h2 id="speed-test">Speed test</h2>
<p>So. Mid-2014 MacBook Pro, Matlab R2016a. In a test harness, expanding each column of a 10Ã—1000 input array between 0 and 39 times,</p>
<ul>
<li>the initial code written by colleague came in at 5.1 milliseconds.</li>
<li>The stupidly-simple reallocate-every-iteration approach aiming at clarity: <em>1.7Ã—</em> <strong>faster</strong>.</li>
<li>The spicy-ğŸŒ¶ using <code>cumsum</code> (slightly wrong): <strong>15Ã— faster</strong>.</li>
<li>The ğŸŒ¶ğŸŒ¶ğŸŒ¶ tweak on the above, to be absolutely correct: <strong>9.9Ã— faster</strong>.</li>
</ul>
<p>Lesson 1, for all programmers: reallocating might be fine, even the Matlab linter canâ€™t break Alexandrescuâ€™s Dictum. â€œYou must measure everything.â€</p>
<p>Lesson 2, for my numerical colleagues: when working through the nitty-gritty indexing-and-shifting details of an implementation, formal analysis is <em>always</em> good, but try getting it to work on one example and tweak till it works for the general case.</p>
<p>(Banner credit: Lewis Hineâ€™s <em>Photograph of a Workman on the Framework of the Empire State Building</em>, 1930. <a href="https://catalog.archives.gov/id/518290">United States National Archives</a>. <a href="https://commons.wikimedia.org/wiki/File:Old_timer_structural_worker2.jpg">Wikimedia</a>.)</p>
<p>
<small>Previous: <a href="/post/texshade/">Texture-shaded Globe</a><br>Next: <a href="/post/spherical-cap/">Random points on a spherical cap</a></small>
</p>
